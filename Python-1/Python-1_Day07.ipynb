{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P58 057：re1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re匹配IP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(0, 15), match='202.196.208.111'>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 正则如何匹配ip\n",
    "import re \n",
    "\n",
    "# for i in range(255):\n",
    "#     print(str(i)+\":\"+ str(re.search(r\"([2][5][0-5]|[2][0-4]\\d|[1]\\d{2}|\\d{1,2})\",str(i)).group()))  # 位数少的\"\\d{1,2}\"放在后面\n",
    "re.search(r\"(([2][5][0-5]|[2][0-4]\\d|[1]\\d{2}|\\d{1,2})\\.){3}([2][5][0-5]|[2][0-4]\\d|[1]\\d{2}|\\d{1,2})\", \"202.196.208.111\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P59 058：re2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# re的各种特殊符号：^ $ () * + ?\n",
    "# 贪婪和非贪婪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HelloByeByeHello\n",
      "HelloByeHello\n",
      "HelloByeBye\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.search(r\"^Hello\", \"1Hello\")  # ^匹配首起始字符：\\A同效果\n",
    "re.search(r\"Hello$\", \"Hello0\")  # $匹配结尾字符：\\Z同效果\n",
    "# \\1表示重复正则第1个圆括号内匹配到的内容\\2表示重复正则第2个圆括号内匹配到的内容\n",
    "# \\1-99是编号，\\100以后作为八进制算\n",
    "print(re.search(r\"^(Hello)(Bye)\\2\\1$\", \"HelloByeByeHello\").group())  # \\1\\2必须放在最后\n",
    "print(re.search(r\"^(Hello)(Bye)\\1$\", \"HelloByeHello\").group())\n",
    "print(re.search(r\"^(Hello)(Bye)\\2$\", \"HelloByeBye\").group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H\n",
      ".\n",
      "['H', '.', 'B']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(re.search(r\".\", \"Hello.Bye\").group())  # .匹配出换行符外的所有字符\n",
    "print(re.search(r\"[.]\", \"Hello.Bye\").group())  # [.]中括号，字符类，匹配里面的字符\n",
    "# print(re.search(r\"[\\]\", \"Hello\\Bye\").group())  # [\\]并不会匹配\\，会报错\n",
    "print(re.findall(r\"[^a-z]\", \"Hello.Bye\"))  # ^放在[]起始的位置，表示取反\n",
    "# *（更简洁、效率，推荐）:匹配前面的子表达式，零次或多次，等价于{0,}\n",
    "# +（更简洁、效率，推荐）:匹配前面的子表达式，一次或多次，等价于{1,}\n",
    "# ?（更简洁、效率，推荐）:匹配前面的子表达式，零次或一次，等价于{0,1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html><head>标题</head><div>内容</div></html>\n",
      "<html>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(re.search(r\"<.+>\", \"<html><head>标题</head><div>内容</div></html>\").group())  # 贪婪\n",
    "print(re.search(r\"<.+?>\", \"<html><head>标题</head><div>内容</div></html>\").group())  # 非贪婪模式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P60 059：re3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# findall\n",
    "# compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'Hello', 'Hello', 'Hello']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# 中间的两个没有匹配，因为_当做字母处理，所以方法和变量名可以加_，不能用空格\n",
    "print(re.findall(r\"\\bHello\\b\", \"Hello Hello?Hello_Hello.Hello(Hello)\"))  # \\b匹配单词边界，\\B匹配非单词边界\n",
    "# \\s:匹配Unicode中的空白字符（[\\t\\n\\r\\f\\v]等）\\f换页\\v垂直换行\n",
    "# \\S:匹配非Unicode中的空白字符\n",
    "# \\w:匹配任何Unicode中的单词字符，包括数字和下划线，等价于[a-zA-Z0-9_]\n",
    "# \\W:匹配任何非Unicode中的单词字符，包括数字和下划线，等价于[a-zA-Z0-9_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['H', 'e', 'l', 'l', 'o', '你', '好', '1', '1', '0', '_', '1', '1', '9']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "p = re.compile(\"\\w\")\n",
    "p.findall(\"Hello?你好:110_119！\")  # w匹配：字母、数字、下划线、汉字等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P61 060：re4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group\n",
    "# re在爬虫中的应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everyone 1 15 (1, 15)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "p = re.compile(\"(\\w+) (\\w+)\")\n",
    "result = p.search(\" Hello Everyone! ByeBye\")\n",
    "print(result.group(2), result.start(), result.end(), result.span())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def download_img():  \n",
    "#     url = \"http://pic.netbian.com/4kmeinv/\"  # 会报UnicodeDecodeError\n",
    "    url = \"http://www.win4000.com/meinv75690_5.html\"  # 不会报UnicodeDecodeError\n",
    "    html = url_open(url).decode(\"utf-8\")  # UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position\n",
    "    get_imgs(html)\n",
    "\n",
    "def get_imgs(html):\n",
    "    soup = BeautifulSoup(html, \"lxml\") \n",
    "    sub_html = soup.find(class_ = \"Left_bar\")\n",
    "    p = r'data-original=\"([^\"]+\\.jpg)\"'  # [^\"]+匹配除\"外的所有字符一次或多次；\n",
    "    # 如果正则中匹配的有元组（即小括号包含的内容），则把元组单独返回，如果有多个，则组合后返回\n",
    "    img_list = re.findall(p, str(sub_html))\n",
    "    for img_url in img_list:\n",
    "        filename = img_url.split(\"/\")[-1]  # 分割后的数组，取最后一个为文件名称     \n",
    "        urllib.request.urlretrieve(img_url, filename, None)           \n",
    "# urlretrieve()方法直接将远程数据下载到本地。\n",
    "# urlretrieve(url, filename=None, reporthook=None, data=None)\n",
    "# 参数url：下载链接地址\n",
    "# 参数filename：指定了保存本地路径（如果参数未指定，urllib会生成一个临时文件保存数据。）\n",
    "# 参数reporthook：是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度。\n",
    "# 参数data：指post导服务器的数据，该方法返回一个包含两个元素的(filename, headers) 元组，\n",
    "# filename 表示保存到本地的路径，header表示服务器的响应头\n",
    "\n",
    "\n",
    "def url_open(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0\")\n",
    "    \n",
    "    response = urllib.request.urlopen(req)\n",
    "    return response.read()\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    download_img()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59.57.148.64 113.195.170.95 123.138.107.200 119.146.131.186 113.121.22.33 120.83.107.120 183.154.51.166 120.83.106.218 120.83.100.191 49.89.143.223 183.164.238.18 1.198.73.218 117.57.90.72 27.152.91.126 171.11.32.63 182.34.33.133 120.83.111.85 113.121.23.235 123.163.122.33 182.35.85.223 112.250.107.37 61.130.181.114 119.123.179.30 14.20.235.180 14.115.105.125 222.249.238.138 27.128.187.22 120.78.225.5 163.125.148.91 61.128.208.94 140.143.48.49 59.44.247.194 219.131.243.230 101.254.136.130 58.20.37.25 120.25.253.234 123.59.211.215 180.150.185.191 118.26.170.209 58.243.50.184 119.146.131.186 113.121.22.33 120.83.107.120 120.83.100.191 49.89.143.223 182.34.33.133 113.121.23.235 182.35.85.223 112.111.77.68 60.13.42.226 112.250.107.37 58.253.153.204 113.128.8.207 182.34.35.124 171.11.29.6 123.169.103.108 114.239.150.215 61.130.181.114 114.239.151.156 27.157.129.87 59.57.148.64 113.195.170.95 123.138.107.200 183.154.51.166 120.83.106.218 183.164.238.18 1.198.73.218 117.57.90.72 27.152.91.126 171.11.32.63 120.83.111.85 123.163.122.33 114.239.253.174 60.13.42.184 118.212.107.148 222.184.7.206 114.239.147.101 182.35.84.252 27.43.189.220 27.152.91.6 58.253.156.91 120.83.111.155 117.95.175.157 120.83.104.222 120.83.105.227 115.53.20.82 121.63.198.84 121.31.150.165 121.31.193.132 182.33.217.116 117.69.98.216 121.31.103.33 113.121.245.231 113.121.245.32 218.73.130.59 110.73.33.207 110.73.30.246 112.114.78.54 171.38.64.67 112.114.76.176 "
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import re\n",
    "\n",
    "def get_ips(html):\n",
    "    # 返回的都是元组\n",
    "    # p = r\"(([2][5][0-5]|[2][0-4]\\d|[1]\\d{2}|\\d{1,2})\\.){3}([2][5][0-5]|[2][0-4]\\d|[1]\\d{2}|\\d{1,2})\"\n",
    "    # 所有子组的前面添加?:\n",
    "    p = r\"(?:(?:[2][5][0-5]|[2][0-4]\\d|[1]\\d{2}|\\d{1,2})\\.){3}(?:[2][5][0-5]|[2][0-4]\\d|[1]\\d{2}|\\d{1,2})\"\n",
    "    ip_list = re.findall(p, html)\n",
    "    for ip in ip_list:\n",
    "        print(ip, end=\" \")           \n",
    "\n",
    "def url_open(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0\")\n",
    "    \n",
    "    response = urllib.request.urlopen(req)\n",
    "    return response.read().decode(\"utf-8\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.xicidaili.com/\"\n",
    "    get_ips(url_open(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P62 061：异常"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spider中两种异常的写法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reason: [Errno 11004] getaddrinfo failed\n"
     ]
    }
   ],
   "source": [
    "# HTTPError是URLError的子类：\n",
    "# StatusCode:100-299表示成功；400-499表示客户端出现异常；500-599表示服务器出现异常；\n",
    "# 第一种异常写法\n",
    "import urllib.request\n",
    "from urllib.error import URLError, HTTPError\n",
    "\n",
    "def url_open(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0\")\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "    except HTTPError as e:  # 要写在URLError的前面，才会响应到\n",
    "        print(\"Error Code:\", e.code)\n",
    "    except URLError as e:\n",
    "        print(\"Reason:\", e.reason)\n",
    "    else:\n",
    "        return response.read().decode(\"utf-8\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    url = \"https://www.daili.com/\"  # 胡乱输入测试\n",
    "    url_open(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We failed to reach a server.\n",
      "Reason: Not Found\n"
     ]
    }
   ],
   "source": [
    "# 第二种异常写法（推荐）\n",
    "import urllib.request\n",
    "from urllib.error import URLError, HTTPError\n",
    "\n",
    "def url_open(url):\n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"User-Agent\", \"Mozilla/5.0 (Windows NT 6.1; Win64; x64; rv:69.0) Gecko/20100101 Firefox/69.0\")\n",
    "    try:\n",
    "        response = urllib.request.urlopen(req)\n",
    "    except URLError as e:\n",
    "        if hasattr(e, \"reason\"):\n",
    "            print(\"We failed to reach a server.\")\n",
    "            print(\"Reason:\", e.reason)\n",
    "        elif hasattr(e, \"code\"):\n",
    "            print(\"The server could\\'t fulfill the request.\")\n",
    "            print(\"Error Code:\", e.code)\n",
    "    else:\n",
    "        # print(response.read().decode(\"utf-8\"))  # UnicodeDecodeError:\n",
    "        # print(response.read().decode(\"utf-8\", \"ignore\"))  # 中文乱码\n",
    "        print(response.read().decode(\"gbk\"))\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    url = \"http://pic.netbian.com/3kmeinv/\"  # 胡乱输入测试\n",
    "    url_open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P64 063：Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用Scrapy抓取网站的四个步骤：\n",
    "# 1.创建一个Scrapy项目；\n",
    "# 2.定义Item容器；\n",
    "# 3.编写爬虫；\n",
    "# 4.存储内容；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编写爬虫\n",
    "# 1.编写dmoz_spider.py\n",
    "# 2.cmd中安装pip install pywin32\n",
    "# 3.cmd中运行scrapy crawl dmoz_spider \n",
    "# 然后是如何跟进网页中的链接以及如何分析页面中的内容，还有提取生成item的方法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selectotr是个选择器，有4个基本方法：\n",
    "# xpath():传入xpath表达式，返回表达式所对应的所有节点的selector list。\n",
    "# css():传入css表达式，返回表达式所对应的所有节点的selector list。\n",
    "# extract():序列化该节点为unicode字符串并返回list。\n",
    "# re():根据出入的正则表达式对数据进行提取，返回unicode字符串list。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmd控制台：\n",
    "# spider shell \"http://www.chinadmoz.org/\"\n",
    "# response.body\n",
    "# response.xpath(\"//li\").extract()  # //:获取页面中所有对应标签的list，类型是object;extract:返回list\n",
    "# sel.xpath(\"//ul/li/a/text()\").extract()  # 获取a标签中的文本\n",
    "# sel.xpath(\"//ul/li/a/@href\").extract()  # 获取a标签中的网站\n",
    "# exit()  # 退出xpath模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cmd控制台：\n",
    "# 到处数据共四种格式：json、json line、csv、xml\n",
    "# scrapy crawl dmoz -o item.json -t json  # -o 后边是导出的文件名，-t 指定导出类型\n",
    "# scrapy crawl dmoz -o item.json -s FEED_EXPORT_ENCODING=UTF-8\n",
    "# 在settings.py文件中加入FEED_EXPORT_ENCODING='UTF-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q & A：\n",
    "# Q:Forbidden by robots.txt\n",
    "# A:setting.py ROBOTSTXT_OBEY = True 改成False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
